{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', -1)\n",
    "pd.set_option('max_colwidth', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = Path('../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_2020-12-27_19-36.json     offenseval-gr-labela-v1.csv\r\n",
      "data_2021-01-02_15-42.json     offenseval-gr-test-v1.tsv\r\n",
      "data.json\t\t       offenseval-gr-training-v1.tsv\r\n",
      "labels-levela.csv\t       offenseval-tr-labela-v1.tsv\r\n",
      "offenseval-ar-labela-v1.csv    offenseval-tr-testset-v1.tsv\r\n",
      "offenseval-ar-test-v1.tsv      offenseval-tr-training-v1.tsv\r\n",
      "offenseval-ar-training-v1.tsv  olid-training-v1.0.tsv\r\n",
      "offenseval-da-test-v1.tsv      original_zips\r\n",
      "offenseval-da-training-v1.tsv  testset-levela.tsv\r\n"
     ]
    }
   ],
   "source": [
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng = pd.read_csv(data_dir/\"olid-training-v1.0.tsv\", sep='\\t')\n",
    "eng = eng.drop(['subtask_b', 'subtask_c'], axis=1).rename(columns={'subtask_a': 'label'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_t_ex = pd.read_csv(data_dir/'testset-levela.tsv', sep='\\t').set_index('id')\n",
    "\n",
    "eng_t_labels = pd.read_csv(data_dir/'labels-levela.csv', header=None, names=['id', 'label']).set_index('id')\n",
    "\n",
    "eng_test = eng_t_ex.join(eng_t_labels,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14100"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(eng)+ len(eng_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## other langs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "LANGS = list(set([str(i).split('-')[1] for i in data_dir.glob('offens*')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3515 3528 3515\n",
      "1827 2000 1827\n",
      "1544 1544 1544\n"
     ]
    }
   ],
   "source": [
    "data = {}\n",
    "for lang in LANGS:\n",
    "    data.setdefault(lang, {})\n",
    "    lang_files = [i for i in data_dir.glob(f'*{lang}*')]\n",
    "    \n",
    "    train_file = [i for i in lang_files if 'train' in str(i)][0]\n",
    "    assert str(train_file).endswith('.tsv')\n",
    "    train = pd.read_csv(train_file, sep='\\t').rename(columns={'subtask_a': 'label'})\n",
    "    \n",
    "    if lang == 'da':\n",
    "        train =train.iloc[:-1]\n",
    "        data[lang]['train'] = train\n",
    "\n",
    "        test_file = [i for i in lang_files if 'test' in str(i)][0]\n",
    "        test_data = pd.read_csv(test_file, sep='\\t').rename(columns={'subtask_a': 'label'})\n",
    "        data[lang]['test'] = test_data\n",
    "        continue\n",
    "        \n",
    "    data[lang]['train'] = train\n",
    "\n",
    "    test_label_file = [i for i in lang_files if 'labela' in str(i)][0]\n",
    "    \n",
    "    if str(test_label_file).endswith('.tsv') and lang != 'tr':\n",
    "        test_labels = pd.read_csv(test_label_file, sep='\\t', header=None, names=['id', 'label'])\n",
    "    else:\n",
    "        test_labels = pd.read_csv(test_label_file, header=None, names=['id', 'label'])\n",
    "        \n",
    "    test_labels = test_labels.set_index('id')\n",
    "\n",
    "    test_examples_file = [i for i in lang_files if'test' in str(i)][0]\n",
    "    assert str(test_examples_file).endswith('.tsv') \n",
    "    \n",
    "    test_examples = pd.read_csv(test_examples_file, sep='\\t').set_index('id')\n",
    "\n",
    "    test_data = test_examples.join(test_labels,)\n",
    "    print(len(test_examples), len(test_labels), len(test_data))\n",
    "    \n",
    "    data[lang]['test'] = test_data.reset_index()\n",
    "\n",
    "    \n",
    "data.setdefault('en', {})\n",
    "data['en']['train'] = eng\n",
    "data['en']['test'] = eng_test.reset_index()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def dump_data(data_dict, filename=f\"data_{datetime.today().strftime('%Y-%m-%d_%H-%M')}\"):\n",
    "    print(filename)\n",
    "    json.dump({i: {'train': v['train'].to_json(),\n",
    "                   'test': v['test'].to_json(),}\n",
    "                    for i,v in data.items()\n",
    "              },\n",
    "              open(data_dir/f'{filename}.json', 'w+')) \n",
    "              \n",
    "def read_data(json_path):\n",
    "    data = json.load(open(json_path, 'r'))\n",
    "    for key, lang_dict in data.items():\n",
    "        for k, v in lang_dict.items():\n",
    "            data[key][k] = pd.read_json(v)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data(data_dir/\"data_2020-12-27_19-36.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_2021-02-02_23-55\n"
     ]
    }
   ],
   "source": [
    "dump_data(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from wordsegment import load, segment\n",
    "import emoji\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def eng_segment_hashtags(s):\n",
    "    load()\n",
    "    hashtags = set(part for part in s.split() if part.startswith('#'))\n",
    "    for hashtag in hashtags:\n",
    "        segmented = \" \".join(segment(hashtag))\n",
    "        s = s.replace(hashtag, segmented)\n",
    "    return s\n",
    "\n",
    "def preprocess_tweet(tweet):\n",
    "    tweet = tweet.replace(':', ' ')\n",
    "    tweet = ' '.join(tweet.split()) # remove multiple spaces\n",
    "    \n",
    "    tweet = re.sub(r'(@USER ){2,}', r'\\1', tweet) # replace multiple User mentions with one only\n",
    "    tweet = re.sub(r'(#(\\S\\w*))',r'\\2', tweet) # remove hashtags signs\n",
    "    tweet = re.sub(r'(_){2,}', r'\\1', tweet).replace('_', ' ')  # replace underscores with spaces\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tr\n",
      "____train\n",
      "____test\n",
      "da\n",
      "____train\n",
      "____test\n",
      "ar\n",
      "____train\n",
      "____test\n",
      "gr\n",
      "____train\n",
      "____test\n",
      "en\n",
      "____train\n",
      "____test\n"
     ]
    }
   ],
   "source": [
    "for lang, lang_dict in data.items():\n",
    "    print(lang)\n",
    "    for type_, df in lang_dict.items():\n",
    "        print(f\"____{type_}\")\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "#         df['tweet'] = df['tweet'].apply(lambda x: emoji.demojize(x))\n",
    "        df['tweet'] = df['tweet'].apply(lambda x: preprocess_tweet(x))\n",
    "        data[lang][type_] = df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Not used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize.stanford_segmenter import StanfordSegmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/zeina/documents/nlp_project/dependencies/stanford-segmenter-4.2.0/stanford-segmenter-2020-11-17/data/'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ[\"STANFORD_MODELS\"] = \"/home/zeina/Documents/nlp_project/dependencies/stanford-segmenter-4.2.0/stanford-segmenter-2020-11-17/data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zeina/anaconda3/envs/nlp/lib/python3.7/site-packages/ipykernel_launcher.py:1: DeprecationWarning: \n",
      "The StanfordTokenizer will be deprecated in version 3.2.5.\n",
      "Please use \u001b[91mnltk.parse.corenlp.CoreNLPTokenizer\u001b[0m instead.'\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<', ' ', 'L', 'F', ' ', '>', ' ', '<', ' ', 'L', 'F', ' ', '>', '#', ' ', 'ب', ' ', 'ا', 'ل', 'م', 'ل', 'ل', 'ي', 'م', 'ي', 'ت', 'ر', 'ي', 'ا', 'ح', 'ب', 'ي', 'ب', 'ي', ' ', 'U', 'R', 'L', '\\n']\n"
     ]
    }
   ],
   "source": [
    "seg = StanfordSegmenter(path_to_jar='dependencies/stanford-segmenter-4.2.0/stanford-segmenter-2020-11-17/stanford-segmenter-4.2.0.jar')\n",
    "seg.default_config('ar')\n",
    "sent = \"<LF> <LF>#بالملليميتر_يا_حبيبي URL\t\"\n",
    "print(seg.segment(sent.split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
